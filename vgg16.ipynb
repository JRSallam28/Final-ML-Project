{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import glob as gb\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "print(\"Packages imported...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-import dataset\n",
    "data='C:/Users/wolfw/OneDrive/Desktop/Machine Learning/Project/Final Project/DataSet/asl_alphabet_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in  os.listdir(data) : \n",
    "    pathname= str( data +'/'+ folder)\n",
    "    files = gb.glob(pathname)\n",
    "    print(f'For training data , found {len(files)} in folder {folder}')\n",
    "    print(pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in  os.listdir(data) : \n",
    "    pathname= str( data +'/'+ folder)\n",
    "    files = gb.glob(pathname)\n",
    "    print(f'For training data , found {len(files)} in folder {folder}')\n",
    "    print(pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty list to contain images itself called featues for the CNN model  \n",
    "X_data = []\n",
    "#empty list to contain actual value for each image\n",
    "y_data = []\n",
    "for folder in  os.listdir(data) : \n",
    "    pathname= str( data +'/'+ folder)\n",
    "    files = gb.glob(os.path.join(pathname, '*'))\n",
    "    for file in files: \n",
    "        image = cv2.imread(file)\n",
    "        #using cv2.resize without determine interpolation make it preserve aspect ratio for each image \n",
    "        image_array = cv2.resize(image , (s,s))\n",
    "        X_data.append(list(image_array))\n",
    "        y_data.append(code[folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2,random_state=42,stratify=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.layers import Activation, Dense,Dropout\n",
    "from tensorflow.keras import layers, models, Model, optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
    "vgg_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(128, 128,3),classes=29)# include top to play on layers\n",
    "\n",
    "for layer in vgg_model.layers[10:13]: ##control layer\n",
    "    layer.trainable = False\n",
    "   \n",
    "for i, layer in enumerate(vgg_model.layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "\n",
    "\n",
    "x = vgg_model.output\n",
    "x = Flatten()(x) # Flatten dimensions to for use in FC layers\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x) # Dropout layer to reduce overfitting\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(29, activation='softmax')(x) # Softmax for multiclass1\n",
    "transfer_model = Model(inputs=vgg_model.input, outputs=x)\n",
    "\n",
    "\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_accuracy', factor=0.6, patience=8, verbose=1, mode='max', min_lr=5e-5)\n",
    "checkpoint = ModelCheckpoint('vgg16_finetune.h15.keras', monitor= 'val_accuracy', mode= 'max', save_best_only = True, verbose= 1)\n",
    "\n",
    "def VGG16x(Train, labelTrain, Test, labelTest):\n",
    "  learning_rate= 5e-5\n",
    "  transfer_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizers.Adam(learning_rate=5e-5), metrics=[\"accuracy\"])\n",
    "  history = transfer_model.fit(Train, labelTrain, batch_size = 16, epochs=10, validation_data=(Test,labelTest), callbacks=[lr_reduce,checkpoint])\n",
    "  transfer_model.save(\"VGG16.h5\", include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16x(X_train,y_train,X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
